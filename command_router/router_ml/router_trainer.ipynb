{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9605ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sbuca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sbuca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sbuca\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec #Word2Vec is mostly used for huge datasets\n",
    "\n",
    "\n",
    "#Functions to clean training data\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def detokenize(command):\n",
    "    #Merge string back together after splitting\n",
    "    return TreebankWordDetokenizer().detokenize(command)\n",
    "\n",
    "def clean_command(command):\n",
    "    \"\"\"Cleans the incoming command by removing email and punctuation and sets to lowercase\"\"\"\n",
    "    \n",
    "    #Remove email and join string back togehter \n",
    "    clean_string = [word for word in command.split() if '@' not in word]\n",
    "    clean_string = detokenize(clean_string)\n",
    "\n",
    "    #Remove punctuation from string\n",
    "    for char in string.punctuation:\n",
    "        if char in clean_string:\n",
    "            clean_string = clean_string.replace(char, '')\n",
    "\n",
    "    return clean_string.lower()\n",
    "\n",
    "\n",
    "def lemmatize(command):\n",
    "    tokens = command.split()\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "    lemmatized_tokens = []\n",
    "    \n",
    "    #Must lemmatize nouns and verb seperatley because nltk cant do both at once\n",
    "    for token, pos in tagged_tokens:\n",
    "        if pos.startswith('N'): \n",
    "            lemma = lemmatizer.lemmatize(token, pos='n')\n",
    "            \n",
    "        elif pos.startswith('V'): \n",
    "            lemma = lemmatizer.lemmatize(token, pos='v')\n",
    "            \n",
    "        else:\n",
    "            lemma = token\n",
    "            \n",
    "        lemmatized_tokens.append(lemma)\n",
    " \n",
    "    return detokenize(lemmatized_tokens)\n",
    "\n",
    "\n",
    "def command_prepper(command):\n",
    "    \"\"\"Handles all function neccesary to prepping a raw command\n",
    "        to be converted into a vector\"\"\"\n",
    "    \n",
    "    cleaned_command = clean_command(command)\n",
    "    lemmatized_command = lemmatize(cleaned_command)\n",
    "    \n",
    "    return lemmatized_command\n",
    "\n",
    "\n",
    "#command_prepper(\"Hey... howare yall.,.!! 123@mgail.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "905d43a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67a772cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"model_data.xlsx\", sheet_name=\"data\")\n",
    "\n",
    "#Preprocessor words to numbers\n",
    "data['clean_text'] = data['input'].apply(lambda x: command_prepper(x))\n",
    "data['clean_text_tok']=[nltk.word_tokenize(i) for i in data['clean_text']]\n",
    "\n",
    "\n",
    "#Initiate model\n",
    "model = Word2Vec(data['clean_text_tok'],min_count=1) \n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))\n",
    "\n",
    "\n",
    "#Split Data\n",
    "X_train, X_val, y_train, y_val = train_test_split(data[\"clean_text\"],\n",
    "                                                  data[\"output\"],\n",
    "                                                  test_size=0.2,\n",
    "                                                  shuffle=True)\n",
    "X_train_tok= [nltk.word_tokenize(i) for i in X_train]  #for word2vec\n",
    "X_val_tok= [nltk.word_tokenize(i) for i in X_val]      #for word2vec\n",
    "\n",
    "\n",
    "#Token stuff\n",
    "#TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "\n",
    "X_val_vectors_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "\n",
    "#Word2vec\n",
    "# Fit and transform\n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_val_vectors_w2v = modelw.transform(X_val_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5faa27cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.33      0.50         3\n",
      "           2       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.33         3\n",
      "   macro avg       0.50      0.17      0.25         3\n",
      "weighted avg       1.00      0.33      0.50         3\n",
      "\n",
      "Confusion Matrix: [[1 2]\n",
      " [0 0]]\n",
      "AUC: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sbuca\\anaconda33\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\sbuca\\anaconda33\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\sbuca\\anaconda33\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\sbuca\\anaconda33\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:941: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
      "  warnings.warn(\"No negative samples in y_true, \"\n"
     ]
    }
   ],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)\n",
    "#It's a probabilistic classifier that makes use of Bayes' Theorem, a rule that uses probability to make predictions based on prior knowledge of conditions that might be related. This algorithm is the most suitable for such large dataset as it considers each feature independently, calculates the probability of each category, and then predicts the category with the highest probability.\n",
    "\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
    "\n",
    "#Predict y value for test dataset\n",
    "y_predict = nb_tfidf.predict(X_val_vectors_tfidf)\n",
    "y_prob = nb_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n",
    " \n",
    "\n",
    "print(classification_report(y_val,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "861a25dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                             text               clean_text  predict_prob  \\\n",
      "0   1  send and email to 123@gmail.com        send and email to      0.247498   \n",
      "1   2          order dunkin from boost  order dunkin from boost      0.089632   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       3  \n"
     ]
    }
   ],
   "source": [
    "#Testing it on new dataset with the best model\n",
    "df_test=pd.read_excel('forward_testing.xlsx')  #reading the data\n",
    "\n",
    "df_test['clean_text'] = df_test['text'].apply(lambda x: command_prepper(x)) #preprocess the data\n",
    "X_test=df_test['clean_text'] \n",
    "X_vector=tfidf_vectorizer.transform(X_test) #converting X_test to vector\n",
    "y_predict = lr_tfidf.predict(X_vector)      #use the trained model on X_vector\n",
    "y_prob = lr_tfidf.predict_proba(X_vector)[:,1]\n",
    "df_test['predict_prob']= y_prob\n",
    "df_test['target']= y_predict\n",
    "print(df_test.head())\n",
    "final=df_test[['id','target']].reset_index(drop=True)\n",
    "final.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c3a3224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('forward_testing.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d724e97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>send and email to 123@gmail.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>order dunkin from boost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                             text\n",
       "0   1  send and email to 123@gmail.com\n",
       "1   2          order dunkin from boost"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "564e245c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>predict_prob</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>send and email to 123@gmail.com</td>\n",
       "      <td>send and email to</td>\n",
       "      <td>0.247498</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                             text         clean_text  predict_prob  \\\n",
       "0   1  send and email to 123@gmail.com  send and email to      0.247498   \n",
       "\n",
       "   target  \n",
       "0       1  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81adf13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
